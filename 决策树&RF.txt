决策树：非参数的监督学习，解决非参数学习算法、分类问题、多分类问题、回归问题，有非常好的可解释性

决策边界都是横平竖直，平行于X或Y轴

决策树算法主要包括三个部分：特征选择、树的生成、树的剪枝。常用算法有 ID3、C4.5、CART。

熵在信息论中代表，随机变量不确定度的度量。
熵越大，数据的不确定性越高；熵越小，数据的不确定性越低
基尼系数：
信息熵 VS 基尼系数：
1.信息熵的计算比基尼系数稍慢
2.scikit-learn中默认为基尼系数
3.大多数时候二者没有特别的效果优劣
CART超参数：min_samples, min_samples_leaf, min_weight_fraction_leaf,
			max_depth, max_leaf_nodes, min_features

集成学习：通过学习多个弱学习器结合组合策略组成强学习器
集成学习两个代表：Boosting和Bagging
Boosting典型代表GBDT，Bagging典型代表随机森林
梯度提升决策树GBDT，应用场景广泛，准确率高
RF的优势：
	1.容易理解和解释
	2.不需要太多的数据预处理工作
	3.隐含地创造了多个联合特征，并能够解决非线性问题
	4.随机森林模型不容易过拟合
	5.自带out-of-bag (oob)错误评估功能
	6.并行化容易实现
RF的劣势：
	1.不适合小样本，只适合大样本
	2.精度较低
	3.适合决策边界是矩形的，不适合对角线型的

GBDT的优势：
	1.灵活处理各种类型数据
	2.相对于调参少的情况而言，预测的准备率高
	3.使用一些健壮的损失函数，对异常值的鲁棒性非常强。
GBDT劣势：
	1.弱学习器存在依赖关系，难以保存训练
	
GBDT与RF区别：
	相同点：GBDT与RF都是采用多棵树作为最终结果
	不同点：
	1.RF的树可以是回归树也可以是分类树，而GBDT树之间有依赖
	2.RF中树是独立的，相互之间不影响，可以并行；而GBDT中树之间有依赖，是串行
	3.RF最终的结果是多棵树表决决定，而GBDT是多棵树叠加组合最终的结果
	4.RF对异常值不敏感，以为它是多棵树表决；GBDT对异常值比较敏感，错误值会延续给下一棵树
	5.RF是通过减少模型的方差提高性能；而GBDT是减少模型的偏差来提高性能
	6.RF不需要对数据预处理，即特征归一化；GBDT需要进行特征归一化



