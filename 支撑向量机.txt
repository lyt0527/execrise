决策边界的选取：逻辑回归中定义Sigmoid，根据Sigmoid函数进行建模，形成了损失函数，最小化损失函数可以求出决策边界。支撑向量机中决策边界距离最近的点最远

SVM要解决的问题：支撑向量机的目标是找出能够最大化训练集数据间隔（margin）的最优分类超平面。属于监督学习，

1.线性可分(Linearly Separable)：在数据集中，如果可以找出一个超平面，将两组数据分开，那么这个数据集叫做线性可分数据。
2.线性不可分(Linear Inseparable)：在数据集中，没法找出一个超平面，能够将两组数据分开，那么这个数据集就叫做线性不可分数据。
3.分割超平面(Separating Hyperplane)：将数据集分割开来的直线/平面叫做分割超平面。
4.支持向量(Support Vector)：离分割超平面最近的那些点叫做支持向量。
5.间隔(Margin)：支持向量数据点到分割超平面的距离称为间隔

6.KKT条件：KKT条件是泛拉格朗日乘子法的一种形式；主要应用在当我们的优化函数存在不等值约束的情况下的一种最优化求解方式；KKT条件即满足不等式约束情况下的。拉格朗日取得可行解的充要条件。

核函数作用：将非线性可分的数据映射到高维度，使其线性可分